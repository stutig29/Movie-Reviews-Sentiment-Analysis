# -*- coding: utf-8 -*-
"""Simple_ANN_wordembedding

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RHZrrX9toglbLRm47GP6k0ao12WkXjZA
"""

import tensorflow as tf
from tensorflow.keras import layers

print(tf.version.VERSION)
print(tf.keras.__version__)

import numpy as np
import re
import pandas as pd

def preprocess_text(text):
    text= re.sub(r"^\s+|(@[A-Za-z]+)|([^A-Za-z \t])|(,\w+:\/\/\S+)"," ",text)
    text=" ".join(text.split())
    text= text.lower()
    return text

train = pd.read_table("/content/train.tsv")
print(train.columns)
print(train.shape)
test = pd.read_table("/content/test.tsv")
print(test.columns)

print(train.dtypes)

train['Phrase'] = train['Phrase'].apply(lambda x: preprocess_text(x))
test['Sent'] = test['Phrase'].apply(lambda x: preprocess_text(x))

#print(train['Sentiment'].value_counts().plot(kind='bar'))

#print(train.isna().sum())

updated_labels=[]
for i in range(train.shape[0]):
  if train['Sentiment'][i]==0:
    # 1 for negative
    updated_labels.append(1)
  elif train['Sentiment'][i]==4:
    # 3 for positive
    updated_labels.append(3)
  else:
    # 2 for for neutral
    updated_labels.append(train['Sentiment'][i])
train['Sentiment']=updated_labels

print(train['Sentiment'].value_counts().plot(kind='bar'))

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = set(stopwords.words('english'))
def remove_stopwords(text):
    no_stopword_text = [w for w in text.split() if not w in stop]
    return ' '.join(no_stopword_text)

train['Phrase'] = train['Phrase'].apply(lambda x: remove_stopwords(x))
test['Sent'] =  test['Sent'].apply(lambda x: remove_stopwords(x))

for i in range(len(train['Phrase'])):
  if train['Phrase'][i]=='':
    train.drop([i],axis=0,inplace=True)
train.reset_index(inplace=True)
for i in range(len(test['Phrase'])):
  if test['Sent'][i]=='':
    test.drop([i],axis=0,inplace=True)
test.reset_index(inplace=True)

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
def lemmatize(text):
    split=text.split(' ')
    lem_sen=' '.join([lemmatizer.lemmatize(w,'v') for w in split])
    return lem_sen

train['Phrase']= train['Phrase'].apply(lambda x: lemmatize(x))
test['Sent']= test['Sent'].apply(lambda x: lemmatize(x))

from keras.preprocessing.text import Tokenizer
token=Tokenizer()
token.fit_on_texts(train['Phrase'].values)

train['vectors']=token.texts_to_sequences(train['Phrase'])
test['vectors']=token.texts_to_sequences(test['Sent'])
#print(train['vectors'][0:5])

index_of_words = token.word_index
print(len(index_of_words))

from tensorflow.keras.preprocessing.sequence import pad_sequences
len_train = max([len(s.split()) for s in train['Phrase']])
len_test = max([len(s.split()) for s in test['Sent']])
if len_train>len_test:
  max_length = len_train
else:
  max_length = len_test
train_vectors = pad_sequences(train['vectors'], max_length)
test_vectors = pad_sequences(test['vectors'], max_length)
#print(train_vectors.shape)

from keras.utils import to_categorical
target=train.Sentiment.values
labels=to_categorical(target-1)
num_classes=labels.shape[1]
print(labels)
from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val= train_test_split(train_vectors,labels,test_size=0.2,random_state=4)

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

!ls
!pwd

f = open('glove.6B.100d.txt')
embedd_index = {}
for line in f:
    val = line.split()
    word = val[0]
    coff = np.asarray(val[1:],dtype = 'float')
    embedd_index[word] = coff

f.close()
print('Found %s word vectors.' % len(embedd_index))

embedd_index['good']

embed_num_dims = 100
embedding_matrix = np.zeros((len(index_of_words) + 1, embed_num_dims))

tokens = []
labels = []

for word,i in index_of_words.items():
    temp = embedd_index.get(word)
    if temp is not None:
        embedding_matrix[i] = temp
        
#for plotting
        tokens.append(embedding_matrix[i])
        labels.append(word)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding,GlobalAveragePooling1D

EMBEDDING_DIM = 100
unknown = len(token.word_index)+1
model = Sequential()
model.add(Embedding(len(index_of_words) + 1 , embed_num_dims , input_length = max_length , weights = [embedding_matrix]))
model.add(GlobalAveragePooling1D())
model.add(Dense(50, activation = 'relu'))
model.add(Dense(3, activation = 'softmax'))
model.summary()

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(x_train,
                    y_train,
                    epochs = 5,
                    batch_size = 512,
                    validation_data = (x_val,y_val),
                    verbose = 1)

results = model.evaluate(x_train,y_train)
print("Training Accuracy",results[1])

predict = model.predict_classes(test_vectors)

import csv
output = pd.DataFrame(columns=['Phrases','Sentiment'])
sent=[]
label=[]
for i in range(test.shape[0]):
  sent.append(test['Phrase'][i])
  label.append(predict[i]+1)
output['Phrases'] = sent
output['Label'] = label
#output.to_csv("/content/ann_sa.csv")

