{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_wordembedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5HGZFjhe4F1",
        "colab_type": "code",
        "outputId": "d13c0cbe-95ba-4c24-8698-e22d5f99a8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ9RExIxfByt",
        "colab_type": "code",
        "outputId": "6bd6b687-6158-497b-ec71-b364e3ba7a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text= re.sub(r\"^\\s+|(@[A-Za-z]+)|([^A-Za-z \\t])|(,\\w+:\\/\\/\\S+)\",\" \",text)\n",
        "    text=\" \".join(text.split())\n",
        "    text= text.lower()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    split = text.split(' ')\n",
        "    text = ' '.join([lemmatizer.lemmatize(w,'v') for w in split])    \n",
        "    return text\n",
        "    \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    no_stopword_text = [w for w in text.split() if not w in stop]\n",
        "    return ' '.join(no_stopword_text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhfA-MCifLpx",
        "colab_type": "code",
        "outputId": "251d91c2-f1d3-42e6-f47c-06dc998e6eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train = pd.read_table(\"/content/train.tsv\")\n",
        "print(train.columns)\n",
        "print(train.shape)\n",
        "test = pd.read_table(\"/content/test.tsv\")\n",
        "print(test.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')\n",
            "(156060, 4)\n",
            "Index(['PhraseId', 'SentenceId', 'Phrase'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9WLuCHPnBYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['Sent'] = train['Phrase'].apply(lambda x: preprocess_text(x))\n",
        "test['Sent'] = test['Phrase'].apply(lambda x: preprocess_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDUaLhUqnL6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "extra_stopwords = set(['none','high','pow','us','whatever','n','lrb','rrb','b'])\n",
        "stop = stop.union(extra_stopwords)\n",
        "train['Sent'] = train['Sent'].apply(lambda x: remove_stopwords(x))\n",
        "test['Sent'] =  test['Sent'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZmPBUSxn8R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updated_labels=[]\n",
        "for i in range(train.shape[0]):\n",
        "  if train['Sentiment'][i]==0:\n",
        "    # 1 for negative\n",
        "    updated_labels.append(1)\n",
        "  elif train['Sentiment'][i]==4:\n",
        "    # 3 for positive\n",
        "    updated_labels.append(3)\n",
        "  else:\n",
        "    # 2 for for neutral\n",
        "    updated_labels.append(train['Sentiment'][i])\n",
        "train['Sentiment']=updated_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkYtKRoVoHGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train['Sent'])):\n",
        "  if train['Sent'][i]=='':\n",
        "    train.drop([i],axis=0,inplace=True)\n",
        "train.reset_index(inplace=True)\n",
        "for i in range(len(test['Sent'])):\n",
        "  if test['Sent'][i]=='':\n",
        "    test.drop([i],axis=0,inplace=True)\n",
        "test.reset_index(inplace=True)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G-h4ET5oMtR",
        "colab_type": "code",
        "outputId": "1435cbfd-8fbb-4223-b551-6c2dfe8869bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "token=Tokenizer()\n",
        "token.fit_on_texts(train['Sent'].values)\n",
        "train['vectors']=token.texts_to_sequences(train['Sent'])\n",
        "test['vectors']=token.texts_to_sequences(test['Sent'])\n",
        "#print(train['vectors'][0:5])\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "len_train = max([len(s.split()) for s in train['Sent']])\n",
        "len_test = max([len(s.split()) for s in test['Sent']])\n",
        "if len_train>len_test:\n",
        "  max_length = len_train\n",
        "else:\n",
        "  max_length = len_test\n",
        "train_vectors = pad_sequences(train['vectors'], max_length)\n",
        "test_vectors = pad_sequences(test['vectors'], max_length)\n",
        "#print(train_vectors.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iOu1JhTNkr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e12e694-e32a-4e13-b52d-1447c1f871fe"
      },
      "source": [
        "index_of_words = token.word_index\n",
        "print(len(index_of_words))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71bTy5ScosNX",
        "colab_type": "code",
        "outputId": "46682463-127c-4eed-b709-2d1908f9cd2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "target=train.Sentiment.values\n",
        "labels=to_categorical(target-1)\n",
        "num_classes=labels.shape[1]\n",
        "print(labels)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_val,y_train,y_val= train_test_split(train_vectors,labels,test_size=0.2,random_state=4)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu7tN4mUNqah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0bcd6a2-558b-42c8-a29f-444ea371cce9"
      },
      "source": [
        "f = open('glove.6B.100d.txt')\n",
        "embedd_index = {}\n",
        "for line in f:\n",
        "    val = line.split()\n",
        "    word = val[0]\n",
        "    coff = np.asarray(val[1:],dtype = 'float')\n",
        "    embedd_index[word] = coff\n",
        "\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embedd_index))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvvkPB12NtaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_num_dims = 100\n",
        "embedding_matrix = np.zeros((len(index_of_words) + 1, embed_num_dims))\n",
        "\n",
        "tokens = []\n",
        "labels = []\n",
        "\n",
        "for word,i in index_of_words.items():\n",
        "    temp = embedd_index.get(word)\n",
        "    if temp is not None:\n",
        "        embedding_matrix[i] = temp\n",
        "        \n",
        "#for plotting\n",
        "        tokens.append(embedding_matrix[i])\n",
        "        labels.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbSWMBKEcONI",
        "colab_type": "code",
        "outputId": "121b16cd-5ee5-45ee-adad-1c5991bd9396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding,GlobalAveragePooling1D\n",
        "# LSTM\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "#EMBEDDING_DIM = 100\n",
        "unknown = len(token.word_index)+1\n",
        "lstm_model = Sequential()\n",
        "#lstm_model.add(Embedding(unknown, EMBEDDING_DIM, input_length = max_length))\n",
        "lstm_model.add(Embedding(len(index_of_words) + 1 , embed_num_dims , input_length = max_length , weights = [embedding_matrix]))\n",
        "lstm_model.add(LSTM(52,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
        "lstm_model.add(GlobalAveragePooling1D())\n",
        "lstm_model.add(Dense(3, activation = 'softmax'))\n",
        "lstm_model.summary()\n",
        "\n",
        "lstm_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 29, 100)           1238100   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 29, 52)            31824     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 52)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 159       \n",
            "=================================================================\n",
            "Total params: 1,270,083\n",
            "Trainable params: 1,270,083\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahdbD_IUcTTA",
        "colab_type": "code",
        "outputId": "a57086be-9320-4c25-cd3d-02613447e92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "history = lstm_model.fit(x_train,\n",
        "                    y_train,\n",
        "                    epochs = 5,\n",
        "                    batch_size = 512,\n",
        "                    validation_data = (x_val,y_val),\n",
        "                    verbose = 1)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0904 07:42:09.782897 140059901138816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 123544 samples, validate on 30886 samples\n",
            "Epoch 1/5\n",
            "123544/123544 [==============================] - 52s 419us/sample - loss: 0.5625 - acc: 0.7200 - val_loss: 0.5153 - val_acc: 0.7477\n",
            "Epoch 2/5\n",
            "123544/123544 [==============================] - 52s 419us/sample - loss: 0.5020 - acc: 0.7600 - val_loss: 0.4723 - val_acc: 0.7816\n",
            "Epoch 3/5\n",
            "123544/123544 [==============================] - 51s 416us/sample - loss: 0.4671 - acc: 0.7820 - val_loss: 0.4477 - val_acc: 0.7930\n",
            "Epoch 4/5\n",
            "123544/123544 [==============================] - 52s 420us/sample - loss: 0.4440 - acc: 0.7963 - val_loss: 0.4334 - val_acc: 0.7990\n",
            "Epoch 5/5\n",
            "123544/123544 [==============================] - 52s 418us/sample - loss: 0.4269 - acc: 0.8064 - val_loss: 0.4239 - val_acc: 0.8067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhDXMSwxcWyn",
        "colab_type": "code",
        "outputId": "fd5fda02-4b07-45c8-c1d2-ad4e6b67fa53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "results = lstm_model.evaluate(x_train,y_train)\n",
        "print(\"Training Accuracy\",results[1])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123544/123544 [==============================] - 30s 239us/sample - loss: 0.3997 - acc: 0.8213\n",
            "Training Accuracy 0.82132286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lRAY9XcYjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_labels = lstm_model.predict_classes(train_vectors)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob27OapHcaSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "lstm_output = pd.DataFrame(columns=['Phrases','Actual','Predicted'])\n",
        "sent=[]\n",
        "actual_label=[]\n",
        "predicted_label=[]\n",
        "for i in range(train.shape[0]):\n",
        "  sent.append(train['Sent'][i])\n",
        "  actual_label.append(train['Sentiment'][i])\n",
        "  predicted_label.append(predict_labels[i]+1)\n",
        "lstm_output['Phrases'] = sent\n",
        "lstm_output['Actual'] = actual_label\n",
        "lstm_output['Predicted'] = predicted_label\n",
        "#lstm_output.to_csv(\"/content/lstm_sa_test.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Ji7KSvqVhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}